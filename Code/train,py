# 7. 손실 함수 및 옵티마이저 정의 (Weight Decay 추가)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # L2 정규화 (Weight Decay)

# 8. 학습률 감소 스케줄러 정의
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # 7 에폭마다 학습률 10% 감소
#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

# 9. 학습 및 평가 함수 정의
def train(model, train_loader, optimizer):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for data, target in train_loader:
        data, target = data.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

    accuracy = 100. * correct / total
    return running_loss / len(train_loader), accuracy

